---
- name: Prepare routing rules
  hosts: localhost
  connection: local
  vars:
    CMD_LANG:
      LANG: en_US.UTF-8
      LC_MESSAGES: en_US.UTF-8
      LC_ALL: en_US.UTF-8
  tasks:
    - name: Start libvirt
      service:
        name: libvirtd
        state: started
        enabled: yes
    - name: Activate default libvirt network
      virt_net:
        name: default
        state: active
      register: virt_net_out
    - debug: var=virt_net_out
      # all of the next is a workaround for a network issue:
      # vdsm installation breaks the routing by defining separate
      # routing table for ovirtmgmt. But we need to enable communication
      # between virbr0 and ovirtmgmt
    - name: Get libvirt interfaces
      virt_net:
        command: facts
    - name: Get routing rules
      command: ip rule
      environment: "{{ CMD_LANG }}"
      register: route_rules
      changed_when: True
    - debug: var=route_rules
    - name: Save bridge name
      set_fact:
        virbr_default: "{{ ansible_libvirt_networks['default']['bridge'] }}"
    - name: Wait for the bridge to appear on the host
      command: ip link show {{ virbr_default }}
      environment: "{{ CMD_LANG }}"
      changed_when: True
      register: ip_link_show_bridge
      until: ip_link_show_bridge.rc == 0
      retries: 30
      delay: 3
    - name: Refresh network facts
      setup:
      when: virt_net_out.changed
      tags: [ 'skip_ansible_lint' ]
    - name: Prepare CIDR for {{ virbr_default }}
      set_fact:
        virbr_cidr: "{{ (hostvars[inventory_hostname]['ansible_'+virbr_default]['ipv4']['address']+'/'+hostvars[inventory_hostname]['ansible_'+virbr_default]['ipv4']['netmask']) |ipv4('host/prefix') }}"
    - name: Add outbound route rules
      command: ip rule add from {{ virbr_cidr }} priority 101 table main
      environment: "{{ CMD_LANG }}"
      register: result
      when: "\"101:\tfrom \"+virbr_cidr+\" lookup main\" not in route_rules.stdout"
      changed_when: True
    - debug: var=result
    - name: Add inbound route rules
      command: ip rule add from all to {{ virbr_cidr }} priority 100 table main
      environment: "{{ CMD_LANG }}"
      register: result
      changed_when: True
      when: "\"100:\tfrom all to \"+virbr_cidr+\" lookup main\" not in route_rules.stdout"
    - debug: var=result

- name: Create hosted engine local vm
  hosts: localhost
  connection: local
  vars:
    MGMT_NETWORK: ovirtmgmt
    CMD_LANG:
      LANG: en_US.UTF-8
      LC_MESSAGES: en_US.UTF-8
      LC_ALL: en_US.UTF-8
  tasks:
    - include_tasks: validate_hostname_tasks.yml
    - name: Register the engine FQDN as a host
      add_host:
        name: "{{ FQDN }}"
        groups: engine
        ansible_connection: smart
        ansible_ssh_extra_args: -o StrictHostKeyChecking=no
        ansible_ssh_pass: "{{ APPLIANCE_PASSWORD }}"
        ansible_user: root
    - name: Initial tasks
      block:
      - name: Get host unique id
        shell: if [ -e /etc/vdsm/vdsm.id ]; then cat /etc/vdsm/vdsm.id; else dmidecode -s system-uuid; fi;
        environment: "{{ CMD_LANG }}"
        changed_when: True
        register: unique_id_out
      - name: Create directory for local VM
        tempfile:
          state: directory
          path: "{{ LOCAL_VM_DIR_PATH }}"
          prefix: "{{ LOCAL_VM_DIR_PREFIX }}"
        register: otopi_localvm_dir
      - name: Set local vm dir path
        set_fact:
          LOCAL_VM_DIR: "{{ otopi_localvm_dir.path }}"
      - name: Fix local VM directory permission
        file:
          state: directory
          path: "{{ LOCAL_VM_DIR }}"
          owner: vdsm
          group: kvm
          mode: 0775
      - include_tasks: install_appliance.yml
        when: APPLIANCE_OVA is none or APPLIANCE_OVA|length == 0
      - name: Register appliance PATH
        set_fact:
          APPLIANCE_OVA_PATH: "{{ APPLIANCE_OVA }}"
        when: APPLIANCE_OVA is not none and APPLIANCE_OVA|length > 0
      - debug: var=APPLIANCE_OVA_PATH
      - name: Check available space on local VM directory
        shell: df -k --output=avail "{{ LOCAL_VM_DIR }}" | grep -v Avail
        environment: "{{ CMD_LANG }}"
        changed_when: True
        register: local_vm_dir_space_out
      - name: Check appliance size
        shell: gzip -l "{{ APPLIANCE_OVA_PATH }}" | grep -v uncompressed | awk '{print $2}'
        environment: "{{ CMD_LANG }}"
        changed_when: True
        register: appliance_size
      - name: Ensure we have enough space to extract the appliance
        assert:
          that:
            - "local_vm_dir_space_out.stdout_lines[0]|int * 1024 > appliance_size.stdout_lines[0]|int * 1.1"
          msg: >
            {{ LOCAL_VM_DIR }} doesn't provide enough free space to extract the
            engine appliance: {{ local_vm_dir_space_out.stdout_lines[0]|int / 1024 | int }} Mb
            are available while {{ appliance_size.stdout_lines[0]|int / 1024 / 1024 * 1.1 | int }} Mb
            are required.
      - name: Extract appliance to local VM directory
        unarchive:
          src: "{{ APPLIANCE_OVA_PATH }}"
          dest: "{{ LOCAL_VM_DIR }}"
          extra_opts: ['--sparse']
      - include_tasks: get_local_vm_disk_path.yml
      - name: Get appliance disk size
        command: qemu-img info --output=json {{ local_vm_disk_path }}
        environment: "{{ CMD_LANG }}"
        changed_when: True
        register: qemu_img_out
      - debug: var=qemu_img_out
      - name: Parse qemu-img output
        set_fact:
          virtual_size={{ qemu_img_out.stdout|from_json|json_query('"virtual-size"') }}
        register: otopi_appliance_disk_size
      - debug: var=virtual_size
      - name: Create cloud init user-data and meta-data files
        template:
          src: "{{ item.src }}"
          dest: "{{ item.dest }}"
        with_items:
          - { src: templates/user-data.j2, dest: "{{ LOCAL_VM_DIR }}/user-data" }
          - { src: templates/meta-data.j2, dest: "{{ LOCAL_VM_DIR }}/meta-data" }
      - name: Create ISO disk
        command: mkisofs -output {{ LOCAL_VM_DIR }}/seed.iso -volid cidata -joliet -rock -input-charset utf-8 {{ LOCAL_VM_DIR }}/meta-data {{ LOCAL_VM_DIR }}/user-data
        environment: "{{ CMD_LANG }}"
        changed_when: True
      - name: Create local VM
        command: virt-install -n {{ VM_NAME }}Local --os-variant rhel7 --virt-type kvm --memory {{ MEM_SIZE }} --vcpus {{ VCPUS }}  --network network=default,mac={{ VM_MAC_ADDR }},model=virtio --disk {{ local_vm_disk_path }} --import --disk path={{ LOCAL_VM_DIR }}/seed.iso,device=cdrom --noautoconsole --rng /dev/random --graphics vnc --video vga --sound none --controller usb,model=none --memballoon none --boot hd,menu=off --clock kvmclock_present=yes
        environment: "{{ CMD_LANG }}"
        register: create_local_vm
        changed_when: True
      - debug: var=create_local_vm
      - name: Get local VM IP
        shell: virsh -r net-dhcp-leases default | grep -i {{ VM_MAC_ADDR }} | awk '{ print $5 }' | cut -f1 -d'/'
        environment: "{{ CMD_LANG }}"
        register: local_vm_ip
        until: local_vm_ip.stdout_lines|length >= 1
        retries: 50
        delay: 10
        changed_when: True
      - debug: var=local_vm_ip
      - name: Remove leftover entries in /etc/hosts for the local VM
        lineinfile:
          dest: /etc/hosts
          regexp: "# temporary entry added by hosted-engine-setup for the bootstrap VM$"
          state: absent
      - name: Create an entry in /etc/hosts for the local VM
        lineinfile:
          dest: /etc/hosts
          line: "{{ local_vm_ip.stdout_lines[0] }} {{ FQDN }} # temporary entry added by hosted-engine-setup for the bootstrap VM"
          insertbefore: BOF
          backup: yes
      - name: Wait for SSH to restart on the local VM
        local_action:
          module: wait_for
            host='{{ FQDN }}'
            port=22
            delay=30
            timeout=300
      rescue:
        - include_tasks: clean_localvm_dir.yml
        - name: Notify the user about a failure
          fail:
            msg: >
              The system may not be provisioned according to the playbook
              results: please check the logs for the issue,
              fix accordingly or re-deploy from scratch.
- hosts: engine
  vars:
    CLUSTER: Default
    CMD_LANG:
      LANG: en_US.UTF-8
      LC_MESSAGES: en_US.UTF-8
      LC_ALL: en_US.UTF-8
  tasks:
    - name: Initial tasks
      block:
      - name: Wait for the local VM
        wait_for_connection:
          delay: 5
          timeout: 180
      - name: Add an entry for this host on /etc/hosts on the local VM
        lineinfile:
          dest: /etc/hosts
          line: "{{ HOST_IP }} {{ HOST_ADDRESS }}"
      - name: Set FQDN
        command: hostnamectl set-hostname {{ FQDN }}
        environment: "{{ CMD_LANG }}"
        changed_when: True
      - name: Force the local VM FQDN to temporary resolve on the natted network address
        lineinfile:
          path: /etc/hosts
          line: "{{ hostvars['localhost']['local_vm_ip']['stdout_lines'][0] }} {{ FQDN }} # hosted-engine-setup-{{ hostvars['localhost']['LOCAL_VM_DIR'] }}"
      - name: Restore sshd reverse DNS lookups
        lineinfile:
          path: /etc/ssh/sshd_config
          regexp: '^UseDNS'
          line: "UseDNS yes"
      - name: Generate an answer file for engine-setup
        template:
          src: templates/heanswers.conf.j2
          dest: /root/heanswers.conf
          owner: root
          group: root
          mode: 0600
      - name: Include before engine-setup custom tasks files for the engine VM
        include_tasks: "{{ item }}"
        with_fileglob: "hooks/enginevm_before_engine_setup/*.yml"
        register: include_before_engine_setup_results
      - debug: var=include_before_engine_setup_results
      - name: Restore a backup
        block:
        - include_tasks: restore_backup.yml
        when: RESTORE_FROM_FILE is defined and RESTORE_FROM_FILE
      - name: Execute engine-setup
        command: /usr/bin/engine-setup --offline --config-append=/root/ovirt-engine-answers --config-append=/root/heanswers.conf
        environment: "{{ CMD_LANG }}"
        register: engine_setup_out
        changed_when: True
      - debug: var=engine_setup_out
      - name: Include after engine-setup custom tasks files for the engine VM
        include_tasks: "{{ item }}"
        with_fileglob: "hooks/enginevm_after_engine_setup/*.yml"
        register: include_after_engine_setup_results
      - debug: var=include_after_engine_setup_results
      # After a restart the engine has a 5 minute grace time,
      # other actions like electing a new SPM host or reconstructing
      # the master storage domain could require more time
      - name: Wait for the engine to reach a stable condition
        wait_for: timeout=600
        when: RESTORE_FROM_FILE is defined and RESTORE_FROM_FILE
      - name: Configure LibgfApi support
        command: engine-config -s LibgfApiSupported=true --cver=4.2
        environment: "{{ CMD_LANG }}"
        register: libgfapi_support_out
        changed_when: True
        when: ENABLE_LIBGFAPI
      - debug: var=libgfapi_support_out
      - name: Save original OvfUpdateIntervalInMinutes
        shell: "engine-config -g OvfUpdateIntervalInMinutes | cut -d' ' -f2 > /root/OvfUpdateIntervalInMinutes.txt"
        environment: "{{ CMD_LANG }}"
        changed_when: True
      - name: Set OVF update interval to 1 minute
        command: "engine-config -s OvfUpdateIntervalInMinutes=1"
        environment: "{{ CMD_LANG }}"
        changed_when: True
      - name: Restart ovirt-engine service for changed OVF Update configuration and LibgfApi Support
        systemd:
          state: restarted
          name: ovirt-engine
        register: restart_out
      - debug: var=restart_out
      - name: Mask cloud-init services to speed up future boot
        systemd:
          masked: yes
          name: "{{ item }}"
        with_items:
          - cloud-init-local
          - cloud-init
      - name: Clean up bootstrap answer file
        file:
          state: absent
          path: /root/heanswers.conf
      rescue:
        - name: Fetch logs from the engine VM
          include_tasks: fetch_engine_logs.yml
          ignore_errors: yes
        - name: Get local VM dir path
          set_fact:
            LOCAL_VM_DIR={{ hostvars['localhost']['LOCAL_VM_DIR'] }}
        - name: Clean bootstrap VM
          include_tasks: clean_localvm_dir.yml
          delegate_to: localhost
          connection: local
        - name: Notify the user about a failure
          fail:
            msg: >
              There was a failure deploying the engine on the local engine VM.
              The system may not be provisioned according to the playbook
              results: please check the logs for the issue,
              fix accordingly or re-deploy from scratch.
- hosts: localhost
  connection: local
  vars:
    MGMT_NETWORK: ovirtmgmt
    DATA_CENTER: Default
    CLUSTER: Default
    CMD_LANG:
      LANG: en_US.UTF-8
      LC_MESSAGES: en_US.UTF-8
      LC_ALL: en_US.UTF-8
  tasks:
    - name: Add host
      block:
      - name: Wait for ovirt-engine service to start
        uri:
          url: http://{{ FQDN }}/ovirt-engine/services/health
          return_content: yes
        register: engine_status
        until: "'DB Up!Welcome to Health Status!' in engine_status.content"
        retries: 30
        delay: 20
      - debug: var=engine_status
      - name: Detect VLAN ID
        shell: ip -d link show {{ BRIDGE_IF }} | grep 'vlan ' | grep -Po 'id \K[\d]+' | cat
        environment: "{{ CMD_LANG }}"
        register: vlan_id_out
        changed_when: True
      - debug: var=vlan_id_out
      - name: Set Engine public key as authorized key without validating the TLS/SSL certificates
        authorized_key:
          user: root
          state: present
          key: https://{{ FQDN }}/ovirt-engine/services/pki-resource?resource=engine-certificate&format=OPENSSH-PUBKEY
          validate_certs: False
      - include_tasks: auth_sso.yml
      - name: Ensure that the target datacenter is present
        ovirt_datacenter:
          state: present
          name: "{{ DATA_CENTER }}"
          wait: yes
          local: false
          auth: "{{ ovirt_auth }}"
      - name: Ensure that the target cluster is present in the target datacenter
        ovirt_cluster:
          state: present
          name: "{{ CLUSTER }}"
          data_center: "{{ DATA_CENTER }}"
          wait: yes
          auth: "{{ ovirt_auth }}"
      - name: Enable GlusterFS at cluster level
        ovirt_cluster:
          data_center: "{{ DATA_CENTER }}"
          name: "{{ CLUSTER }}"
          auth: "{{ ovirt_auth }}"
          virt: true
          gluster: true
        when: ENABLE_HC_GLUSTER_SERVICE is defined and ENABLE_HC_GLUSTER_SERVICE
      - name: Set VLAN ID at datacenter level
        ovirt_network:
          data_center: "{{ DATA_CENTER }}"
          name: "{{ MGMT_NETWORK }}"
          vlan_tag: "{{ vlan_id_out.stdout }}"
          auth: "{{ ovirt_auth }}"
        when: vlan_id_out.stdout|length > 0
      - name: Force host-deploy in offline mode
        template:
          src: templates/70-hosted-engine-setup.conf.j2
          dest: /etc/ovirt-host-deploy.conf.d/70-hosted-engine-setup.conf
      - name: Add host
        ovirt_host:
          cluster: "{{ CLUSTER }}"
          name: "{{ HOST_NAME }}"
          state: present
          public_key: true
          address: "{{ HOST_ADDRESS }}"
          auth: "{{ ovirt_auth }}"
        async: 1
        poll: 0
      - name: Wait for the host to be up
        ovirt_host_facts:
          pattern: name={{ HOST_NAME }}
          auth: "{{ ovirt_auth }}"
        register: host_result_up_check
        until: host_result_up_check is succeeded and host_result_up_check.ansible_facts.ovirt_hosts|length >= 1 and (host_result_up_check.ansible_facts.ovirt_hosts[0].status == 'up' or host_result_up_check.ansible_facts.ovirt_hosts[0].status == 'non_operational')
        retries: 120
        delay: 5
      - debug: var=host_result_up_check
      - name: Check host status
        fail:
          msg: >
            The host has been set in non_operational status,
            please check engine logs,
            fix accordingly and re-deploy.
        when: host_result_up_check is succeeded and host_result_up_check.ansible_facts.ovirt_hosts|length >= 1 and host_result_up_check.ansible_facts.ovirt_hosts[0].status == 'non_operational'
      - name: Remove host-deploy configuration file
        file:
          state: absent
          path: /etc/ovirt-host-deploy.conf.d/70-hosted-engine-setup.conf
      rescue:
        - name: Fetch logs from the engine VM
          include_tasks: fetch_engine_logs.yml
          ignore_errors: yes
        - include_tasks: clean_localvm_dir.yml
        - name: Notify the user about a failure
          fail:
            msg: >
              The system may not be provisioned according to the playbook
              results: please check the logs for the issue,
              fix accordingly or re-deploy from scratch.
...
