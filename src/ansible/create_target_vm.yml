---
- name: Create hosted engine local vm
  hosts: localhost
  connection: local
  tasks:
  - name: Obtain SSO token with using username/password credentials
    ovirt_auth:
      url: https://{{ FQDN }}/ovirt-engine/api
      username: admin@internal
      insecure: true
      password: "{{ ADMIN_PASSWORD }}"
    register: ovirt_sso_auth
    until: ovirt_sso_auth|succeeded
    retries: 50
    delay: 10
  - name: Get local vm ip
    shell: virsh -r net-dhcp-leases default | grep {{ VM_MAC_ADDR }} | awk '{ print $5 }' | cut -f1 -d'/'
    register: local_vm_ip
    changed_when: True
  - debug: var=local_vm_ip
  - name: Fetch host facts
    ovirt_hosts_facts:
      pattern: name={{ HOST_NAME }} status=up
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: host_result
    until: host_result|succeeded and host_result.ansible_facts.ovirt_hosts|length >= 1
    retries: 50
    delay: 10
  - debug: var=host_result
  - name: Fetch cluster id
    set_fact: cluster_id="{{ host_result.ansible_facts.ovirt_hosts[0].cluster.id }}"
  - name: Fetch cluster facts
    ovirt_cluster_facts:
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: cluster_facts
  - debug: var=cluster_facts
  - name: Fetch datacenter facts
    ovirt_datacenter_facts:
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: datacenter_facts
  - debug: var=datacenter_facts
  - name: Fetch cluster_name
    set_fact: cluster_name="{{ cluster_facts.ansible_facts.ovirt_clusters|json_query(\"[?id=='" + cluster_id + "'].name\")|first }}"
  - name: Fetch datacenter id
    set_fact: datacenter_id="{{ cluster_facts.ansible_facts.ovirt_clusters|json_query(\"[?id=='" + cluster_id + "'].data_center.id\")|first }}"
  - name: Fetch datacenter_name
    set_fact: datacenter_name="{{ datacenter_facts.ansible_facts.ovirt_datacenters|json_query(\"[?id=='" + datacenter_id + "'].name\")|first }}"
  - name: Get storage domain details
    ovirt_storage_domains_facts:
      pattern: name={{ STORAGE_DOMAIN_NAME }} and datacenter={{ datacenter_name }}
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: storage_domain_details
  - debug: var=storage_domain_details
  - name: Add he sanlock disk
    ovirt_disk:
      name: he_sanlock
      size: 1GiB
      format: raw
      interface: virtio
      storage_domain: "{{ STORAGE_DOMAIN_NAME }}"
      timeout: 600
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: he_sanlock_disk_details
  - debug: var=he_sanlock_disk_details
  - name: Add he conf disk
    ovirt_disk:
      name: he_conf
      size: 1GiB
      format: raw
      interface: virtio
      storage_domain: "{{ STORAGE_DOMAIN_NAME }}"
      timeout: 600
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: he_conf_disk_details
  - debug: var=he_conf_disk_details
  - name: Add he metadata disk
    ovirt_disk:
      name: he_metadata
      size: 1GiB
      format: raw
      interface: virtio
      storage_domain: "{{ STORAGE_DOMAIN_NAME }}"
      timeout: 600
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: he_metadata_disk_details
  - debug: var=he_metadata_disk_details
  - name: Add he disk
    ovirt_disk:
      name: he_virtio_disk
      size: 60GiB # TODO: make it parametric
      format: cow
      interface: virtio
      storage_domain: "{{ STORAGE_DOMAIN_NAME }}"
      bootable: true
      timeout: 7200
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: he_virtio_disk_details
  - debug: var=he_virtio_disk_details
  - name: Add VM
    ovirt_vms:
      state: stopped
      cluster: "{{ cluster_name }}"
      name: "{{ VM_NAME }}"
      memory: "{{ MEM_SIZE }}Mib"
      cpu_cores: "{{ VCPUS }}"
      cpu_sockets: 1
      operating_system: rhel_7x64
      type: server
#      timezone: "{{ TIME_ZONE }}" # TODO: fix with the right parameter syntax
      storage_domain: "{{ STORAGE_DOMAIN_NAME }}"
      disks:
      - name: he_virtio_disk
      nics:
      - name: vnet0
        profile_name: "{{ BRIDGE }}"
        interface: virtio
        mac_address: "{{ VM_MAC_ADDR }}"
      auth: "{{ ovirt_sso_auth.ansible_facts.ovirt_auth }}"
    register: he_vm_details
  - debug: var=he_vm_details
  - name: Prepare images
    command: vdsm-client Image prepare storagepoolID={{ datacenter_id }} storagedomainID={{ storage_domain_details.ansible_facts.ovirt_storage_domains[0].id }} imageID={{ item.disk.id }} volumeID={{ item.disk.image_id  }}
    with_items:
      - "{{ he_virtio_disk_details }}"
      - "{{ he_conf_disk_details }}"
      - "{{ he_metadata_disk_details }}"
      - "{{ he_sanlock_disk_details }}"
    register: prepareimage_results
    changed_when: True
  - debug: var=prepareimage_results
  - name: Fetch he conf disk path
    set_fact: he_conf_disk_path="{{ (prepareimage_results.results|json_query(\"[?item.id=='" + he_conf_disk_details.id + "'].stdout\")|first|from_json).path }}"
  - name: Fetch he virtio disk path
    set_fact: he_virtio_disk_path="{{ (prepareimage_results.results|json_query(\"[?item.id=='" + he_virtio_disk_details.id + "'].stdout\")|first|from_json).path }}"
  - name: Fetch he virtio metadata path
    set_fact: he_metadata_disk_path="{{ (prepareimage_results.results|json_query(\"[?item.id=='" + he_metadata_disk_details.id + "'].stdout\")|first|from_json).path }}"
  - debug: var=he_conf_disk_path
  - debug: var=he_virtio_disk_path
  - debug: var=he_metadata_disk_path
  - name: Shutdown local vm
    virt:
      name: "{{ VM_NAME }}Local"
      command: shutdown
      uri: 'qemu+tls://{{ HOST_ADDRESS }}/system'
  - name: Undefine local vm
    virt:
      name: "{{ VM_NAME }}Local"
      command: undefine
      uri: 'qemu+tls://{{ HOST_ADDRESS }}/system'
  - name: Create temp vm conf
    template:
      src: templates/vm.conf.j2
      dest: "{{ LOCAL_VM_DIR }}/vm.conf"
  - name: Create temp broker conf
    template:
      src: templates/broker.conf.j2
      dest: "{{ LOCAL_VM_DIR }}/broker.conf"
  - name: Create version file
    template:
      src: templates/version.j2
      dest: "{{ LOCAL_VM_DIR }}/version"
  - name: Create he answers
    template:
      src: templates/fhanswers.conf.j2
      dest: "{{ LOCAL_VM_DIR }}/fhanswers.conf"
  - name: Create hosted engine conf
    template:
      src: templates/hosted-engine.conf.j2
      dest: "{{ LOCAL_VM_DIR }}/hosted-engine.conf"
  - name: Create configuration archive
    command: tar --record-size=20480 -cvf {{ he_conf_disk_details.disk.image_id }} vm.conf broker.conf version fhanswers.conf hosted-engine.conf
    args:
      chdir: "{{ LOCAL_VM_DIR }}"
    changed_when: True
    tags: [ 'skip_ansible_lint' ]
  - name: Create ovirt-hosted-engine-ha run directory
    file:
      path: /var/run/ovirt-hosted-engine-ha
      state: directory
  - name: Copy vm.conf to the right location on host
    copy:
      src: "{{ LOCAL_VM_DIR }}/vm.conf"
      dest: /var/run/ovirt-hosted-engine-ha
  - name: Copy hosted-engine.conf to the right location on host
    copy:
      src: "{{ LOCAL_VM_DIR }}/hosted-engine.conf"
      dest: /etc/ovirt-hosted-engine/
  - name: Copy configuration archive to storage
    command: dd bs=20480 count=1 oflag=direct if="{{ LOCAL_VM_DIR }}/{{ he_conf_disk_details.disk.image_id }}" of="{{ he_conf_disk_path }}"
    become_user: vdsm
    become_method: sudo
    changed_when: True
    tags: [ 'skip_ansible_lint' ]
  - name: Initialize metadata volume
    command: dd bs=1M count=1024 oflag=direct if=/dev/zero of="{{ he_metadata_disk_path }}"
    become_user: vdsm
    become_method: sudo
    changed_when: True
    tags: [ 'skip_ansible_lint' ]
  - name: Find the local appliance image
    find:
      paths: "{{ LOCAL_VM_DIR }}/images"
      recurse: true
      patterns: ^.*.(?<!meta)$
      use_regex: true
    register: app_img
  - debug: var=app_img
  - name: Copy local vm disk to shared storage
    command: qemu-img convert -n -O qcow2 {{ app_img.files[0].path }} {{ he_virtio_disk_path }}
    become_user: vdsm
    become_method: sudo
    changed_when: True
    tags: [ 'skip_ansible_lint' ]
  - name: Generate network configuration for the engine VM
    template:
      src: templates/ifcfg-eth0-static.j2
      dest: "{{ LOCAL_VM_DIR }}/ifcfg-eth0"
      owner: root
      group: root
      mode: 0644
    when: VM_IP_ADDR|length >= 8
  - name: Inject network configuration with guestfish
    shell: LIBGUESTFS_BACKEND=direct virt-copy-in -a {{ he_virtio_disk_path }} "{{ LOCAL_VM_DIR }}/ifcfg-eth0" /etc/sysconfig/network-scripts
    become_user: vdsm
    become_method: sudo
    when: VM_IP_ADDR|length >= 8
    tags: [ 'skip_ansible_lint' ]
  - name: Extract /etc/hosts from the engine VM
    shell: LIBGUESTFS_BACKEND=direct virt-copy-out -a {{ he_virtio_disk_path }} /etc/hosts "{{ LOCAL_VM_DIR }}"
    become_user: vdsm
    become_method: sudo
    when: not VM_ETC_HOSTS
    tags: [ 'skip_ansible_lint' ]
  - name: Clean /etc/hosts for the engine VM
    lineinfile:
      dest: "{{ LOCAL_VM_DIR }}/hosts"
      line: "{{ HOST_IP }} {{ HOST_ADDRESS }}"
      state: absent
    when: not VM_ETC_HOSTS
  - name: Copy /etc/hosts back to the engine VM
    shell: LIBGUESTFS_BACKEND=direct virt-copy-in -a {{ he_virtio_disk_path }} "{{ LOCAL_VM_DIR }}"/hosts /etc
    become_user: vdsm
    become_method: sudo
    when: not VM_ETC_HOSTS
    tags: [ 'skip_ansible_lint' ]
  - name: Clean /etc/hosts on the host
    lineinfile:
      dest: /etc/hosts
      line: "{{ local_vm_ip.stdout_lines[0] }} {{ FQDN }}"
      state: absent
  - name: Add an entry in /etc/hosts for the target VM
    lineinfile:
      dest: /etc/hosts
      line: "{{ VM_IP_ADDR }} {{ FQDN }}"
      state: present
    when: VM_ETC_HOSTS and VM_IP_ADDR|length >= 8
  - name: Start broker
    service:
      name: ovirt-ha-broker
      state: started
      enabled: true
  - name: Initialize lockspace volume
    command: hosted-engine --reinitialize-lockspace --force
    register: result
    until: result.rc == 0
    retries: 5
    delay: 10
    changed_when: True
  - debug: var=result
  - name: Start agent
    service:
      name: ovirt-ha-agent
      state: started
      enabled: true
...
